{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "number='test'\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 5  # Duration of recording\n",
    "\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "write(f'authenticated_speaker_{number}.wav', fs, myrecording)  # Save as WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fm-pc-lt-233/Personal_Projects/Speech_Ludo_Project/speech_ludo_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/tmp/ipykernel_163540/3210333223.py:1: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import SpeakerRecognition\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "/home/fm-pc-lt-233/Personal_Projects/Speech_Ludo_Project/speech_ludo_env/lib/python3.10/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/home/fm-pc-lt-233/Personal_Projects/Speech_Ludo_Project/speech_ludo_env/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/home/fm-pc-lt-233/Personal_Projects/Speech_Ludo_Project/speech_ludo_env/lib/python3.10/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Embedding Shape: (192,)\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "# Load the pretrained ECAPA-TDNN model\n",
    "verifier = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"tmpdir\")\n",
    "\n",
    "def extract_embedding(audio_path):\n",
    "    \"\"\"Extract speaker embeddings from an audio file.\"\"\"\n",
    "    # Load the audio file\n",
    "    signal, sample_rate = librosa.load(audio_path, sr=16000)  # Ensure 16kHz sampling rate\n",
    "\n",
    "    # Convert to PyTorch tensor and add batch dimension\n",
    "    signal = torch.tensor(signal).unsqueeze(0)  # Shape: [1, num_samples]\n",
    "    \n",
    "    # Extract embedding\n",
    "    embedding = verifier.encode_batch(signal)\n",
    "    return embedding.squeeze().numpy()  # Convert back to NumPy for further processing\n",
    "\n",
    "# Example usage\n",
    "embedding = extract_embedding(\"authenticated_speaker_test.wav\")\n",
    "print(\"Extracted Embedding Shape:\", embedding.shape)  # Should be a 1D vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index contains 3 embeddings.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Initialize FAISS index (L2 normalized embeddings for cosine similarity)\n",
    "embedding_dim = 192  # Dimension of ECAPA-TDNN embeddings\n",
    "index = faiss.IndexFlatIP(embedding_dim)  # IP = Inner Product (cosine similarity with normalized embeddings)\n",
    "\n",
    "# Function to add embeddings to FAISS\n",
    "def add_to_index(embedding, speaker_id, index, metadata):\n",
    "    \"\"\"\n",
    "    Add an embedding to the FAISS index and store metadata.\n",
    "    \"\"\"\n",
    "    embedding = embedding / np.linalg.norm(embedding)  # Normalize for cosine similarity\n",
    "    index.add(np.array([embedding], dtype=np.float32))  # Add to FAISS\n",
    "    metadata.append(speaker_id)  # Save speaker ID for reference\n",
    "\n",
    "# Metadata storage\n",
    "metadata = []\n",
    "number_of_samples=3\n",
    "# Example usage (Add 5 embeddings from authenticated speaker)\n",
    "for i in range(1, number_of_samples+1):\n",
    "    emb = extract_embedding(f\"authenticated_speaker_{i}.wav\")\n",
    "    add_to_index(emb, speaker_id=\"authenticated_user\", index=index, metadata=metadata)\n",
    "\n",
    "print(f\"Index contains {index.ntotal} embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities: [0.6344024  0.55637985 0.36214453]\n",
      "Match Found! Speaker ID: authenticated_user, Similarity: 0.52\n"
     ]
    }
   ],
   "source": [
    "def verify_speaker(test_audio_path,number_of_samples, index, metadata, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Verify if a test speaker matches the authenticated speaker.\n",
    "    \"\"\"\n",
    "    # Extract embedding for the test audio\n",
    "    test_embedding = extract_embedding(test_audio_path)\n",
    "    test_embedding = test_embedding / np.linalg.norm(test_embedding)  # Normalize\n",
    "    \n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(np.array([test_embedding], dtype=np.float32), k=number_of_samples)  # Top 5 matches\n",
    "    mean_similarity = np.mean(distances)  # Average similarity\n",
    "    \n",
    "    print(\"Cosine Similarities:\", distances.flatten())\n",
    "    \n",
    "    # Check if similarity meets the threshold\n",
    "    if mean_similarity > threshold:\n",
    "        speaker_id = metadata[indices[0][0]]\n",
    "        return f\"Match Found! Speaker ID: {speaker_id}, Similarity: {mean_similarity:.2f}\"\n",
    "    else:\n",
    "        return f\"No Match Found. Similarity: {mean_similarity:.2f}\"\n",
    "\n",
    "# Test verification\n",
    "result = verify_speaker(\"authenticated_speaker_test.wav\",number_of_samples, index, metadata,0.5)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
